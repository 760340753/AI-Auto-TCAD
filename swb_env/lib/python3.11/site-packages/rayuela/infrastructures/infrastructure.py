"""Provides an abstract base class for defining infrastructure classes."""


from abc import (
    ABC as _ABC,
    abstractmethod as _abstract_method,
)

from datetime import datetime as _DateTime

from functools import (
    partialmethod as _partialmethod,
    wraps as _wraps,
)

from pathlib import Path as _Path
from socket import gethostname as _get_host_name
from threading import RLock as _RLock

from typing import (
    Callable as _Callable,
    cast as _cast,
    Generator as _Generator,
    Iterable as _Iterable,
    Literal as _Literal,
    Mapping as _Mapping,
    MutableMapping as _MutableMapping,
    Optional as _Optional,
    Tuple as _Tuple,
    Union as _Union,
)

from weakref import (
    finalize as _finalize,
    WeakValueDictionary as _WeakValueDictionary,
)


from dask.base import is_dask_collection as _is_dask_collection  # type: ignore

from distributed import (  # type: ignore
    Client as _Client,
    Future,
)

from distributed.deploy import Cluster as _Cluster  # type: ignore


from ..utils import no_default as _no_default


Fingerprint = int
KWArgsProducer = _Callable[[], _Mapping]
PathOrStr = _Union[_Path, str]
UniqueInstances = _MutableMapping[Fingerprint, "Infrastructure"]


def _safe_future_argument(function: _Callable) -> _Callable:
    @_wraps(function)
    def wrapper(self, futures: Future | _Iterable[Future], *args, **kwargs):
        try:
            futures = _cast(_Iterable[Future], futures)
            assert all(future.client is self._client for future in futures)

        except TypeError:
            try:
                futures = _cast(_Iterable[Future], futures)
                iter(futures)

            except TypeError:
                futures = _cast(Future, futures)
                assert futures.client is self._client

            else:
                raise

        with self._client_lock:
            return function(self, futures, *args, **kwargs)

    return wrapper


class Infrastructure(_ABC):
    """Abstract base class for implementing computing infrastructures.

    .. automethod:: __init__

    .. automethod:: __hash__

    .. automethod:: __eq__
    """

    _unique_instances_lock = _RLock()
    _unique_instances: UniqueInstances = _WeakValueDictionary()

    def init_locks(self):
        """Initialize resource locks.

        This is used when overriding :meth:`__init__`, but you don't
        want to call ``super().__init__()``.
        """
        self._client_lock = _RLock()
        self._cluster_lock = _RLock()

    @_abstract_method
    def __init__(self, _ClientClass=_Client, **kwargs):  # noqa: N803
        """Set up client and cluster.

        :param _ClientClass: The class to use for creating the client.

        :param kwargs:
            Keyword arguments used to support collaborative multiple
            inheritance. These arguments are not used by this class, but
            they are passed to the initializer of the parent class.

        :class:`Infrastructure` is an abstraction over a
        :class:`distributed.Client` and a
        :class:`distributed.deploy.Cluster`. This initializer sets up
        these resources and makes sure they are cleaned up appropriately
        when instances of this class are no longer needed.

        Since the classes inheriting from :class:`Infrastructure` should
        provide a :class:`~distributed.deploy.Cluster` instance via
        :meth:`get_cluster`, this method should be called
        (``super().__init__()``) after initializing the
        :class:`~distributed.deploy.Cluster` instance::

            >>> from unittest.mock import MagicMock
            ...
            >>> class C(Infrastructure):
            ...     def __init__(self):
            ...         # Initialize a cluster instance first!
            ...         self._cluster = MagicMock(name="cluster")
            ...
            ...         # Call the parent method afterwards!
            ...         super().__init__(_ClientClass=MagicMock())
            ...
            ...     @classmethod
            ...     def compute_fingerprint(cls):
            ...         return hash(cls)
            ...
            ...     def __hash__(self):
            ...         return self.compute_fingerprint()
            ...
            ...     def get_cluster(self):
            ...         return self._cluster
            ...
            >>> c = C()
            >>> c.get_cluster()
            <MagicMock name='cluster' id='...'>
        """
        self.init_locks()

        def stop_cluster(client_lock, client, cluster_lock, cluster):
            with client_lock:
                client.close()

            with cluster_lock:
                cluster.close()

        with self._client_lock, self._cluster_lock:
            self._client = _ClientClass(self.get_cluster())

            self.close = _finalize(
                self,
                stop_cluster,
                self._client_lock,
                self._client,
                self._cluster_lock,
                self.get_cluster(),
            )

    @_abstract_method
    def get_cluster(self) -> _Cluster:
        """Return an instance of :class:`distributed.deploy.Cluster`.

        Infrastructures are mainly an abstraction layer over
        :class:`distributed.Client` and
        :class:`distributed.deploy.Cluster`. While all
        :class:`Infrastructure` subclasses should use the same
        :class:`~distributed.Client` class, each :class:`Infrastructure`
        subclass should use a different
        :class:`~distributed.deploy.Cluster` or use one in different
        ways. Subclasses need to override this method to indicate how to
        access the appropriate :class:`~distributed.deploy.Cluster`.
        """
        pass  # pragma: no cover

    @classmethod
    @_abstract_method
    def compute_fingerprint(cls, **kwargs) -> Fingerprint:
        """Compute the fingerprint of this infrastructure.

        :param kwargs:
            Keyword arguments used to support collaborative multiple
            inheritance. These arguments are not used by this class, but
            they are passed to the initializer of the parent class.

        The fingerprint of an infrastructure is an integer that
        identifies infrastructures according to a set of parameters and
        it's class. The recommended way of computing a fingerprint is
        passing the class (the first parameter of this method, usually
        named ``cls``) and all other parameters that control the
        behavior of the class, wrapped in a tuple to the :func:`hash`
        function::

            >>> hash((cls, p1, p2, p3))  # doctest: +SKIP
            129763012

        .. note:: There may be parameters that cannot be hashed, like
            lists or dictionaries. In those cases, a conversion method
            will be needed. For example::

                >>> l = [3, 2, 4, 1]
                >>> t = tuple(l)
                >>> t
                (3, 2, 4, 1)
                >>> d = {'c': 'd', 'a': 'b'}
                >>> e = tuple(sorted(d.items()))
                >>> e
                (('a', 'b'), ('c', 'd'))
                >>> hash((t, e))  # doctest: +SKIP
                4907369275867107361

        When using the :meth:`get_unique` classmethod to instantiate
        infrastructures, the instances are cached by the
        :class:`Infrastructure` class. If :meth:`get_unique` is called
        again with parameters that would produce an equivalent instance,
        the cached instance is returned. Fingerprints are used to
        determine if two instances are equivalent. Only parameters that
        would change the behavior of the instance should be considered
        for computing the fingerprint. e.g. the path to the log files
        does not importantly affect the behavior of an
        :class:`Infrastructure` instance.
        """
        return hash(cls)

    def __eq__(self, other):
        """Return ``True`` if ``other`` has the same hash value as self.

        Infrastructures with the same hash value share the same defining
        parameters. i.e. the parameters involved in defining how this
        infrastructure behave from the user perspective. So, if two
        infrastructures behave the same, they are equivalent for the
        user and should compare equal, even if they have different
        identities.

        When instantiating infrastructures using the class directly
        (``MyInfrastructure(a, b, c)``), calling the class with the same
        parameters will return two different, but equivalent instances.
        When instantiating infrastructures using the
        :meth:`~Infrastructure.get_unique()` method, using the same
        parameters twice will return the same infrastructure.

        ::

            >>> from unittest.mock import MagicMock
            ...
            >>> class C(Infrastructure):
            ...     def __init__(self, a, b):
            ...         self._cluster = MagicMock(name="cluster")
            ...
            ...         self.a = a
            ...         self.b = b
            ...         super().__init__(_ClientClass=MagicMock(name="client"))
            ...
            ...     @classmethod
            ...     def compute_fingerprint(cls, a, b):
            ...         return hash((cls, a, b))
            ...
            ...     def __hash__(self):
            ...         return self.compute_fingerprint(self.a, self.b)
            ...
            ...     def get_cluster(self):
            ...         return self._cluster
            ...
            >>> c1 = C(a=1, b=2)
            >>> c2 = C(a=1, b=2)
            >>> c1 == c2
            True
            >>> c1 is c2
            False
            >>> c1 = C.get_unique(a=1, b=2)
            >>> c2 = C.get_unique(a=1, b=2)
            >>> c1 == c2
            True
            >>> c1 is c2
            True
        """
        return hash(self) == hash(other)

    @_abstract_method
    def __hash__(self):
        """Return the fingerprint of an instantiated Infrastructure.

        For consistent behavior, this method should compute the hash
        using the :meth:`Infrastructure.compute_fingerprint` method
        based on this instance parameters. For example::

            >>> from tempfile import TemporaryDirectory
            >>> from distributed import LocalCluster
            ...
            >>> class C(Infrastructure):
            ...     def __init__(self, a, b):
            ...         self._cluster = LocalCluster(
            ...             dashboard_address=None,
            ...             local_directory=TemporaryDirectory().name
            ...         )
            ...
            ...         self.a = a
            ...         self.b = b
            ...         super().__init__()
            ...
            ...     @classmethod
            ...     def compute_fingerprint(cls, a, b):
            ...         return hash((cls, a, b))
            ...
            ...     def __hash__(self):
            ...         return self.compute_fingerprint(self.a, self.b)
            ...
            ...     def get_cluster(self):
            ...         return self._cluster
            ...
            >>> hash(C(a=1, b=2)) == C.compute_fingerprint(a=1, b=2)
            True

        """
        pass  # pragma: no cover

    @classmethod
    def get_unique(
        cls,
        generate_kwargs: KWArgsProducer = lambda: {},
        **kwargs,
    ) -> "Infrastructure":
        """Return an :class:`Infrastructure` instance.

        This method only creates one instance for each set of
        ``kwargs``. If you call this function with the same ``kwargs``
        twice, it will return the same object::

            >>> from unittest.mock import MagicMock
            ...
            >>> class C(Infrastructure):
            ...     def __init__(self, a, b):
            ...         self._cluster = MagicMock(name="cluster")
            ...
            ...         self.a = a
            ...         self.b = b
            ...         super().__init__(_ClientClass=MagicMock())
            ...
            ...     @classmethod
            ...     def compute_fingerprint(cls, a, b):
            ...         return hash((cls, a, b))
            ...
            ...     def __hash__(self):
            ...         return self.compute_fingerprint(self.a, self.b)
            ...
            ...     def get_cluster(self):
            ...         return self._cluster
            ...
            >>> c1 = C.get_unique(a=1, b=2)
            >>> c2 = C.get_unique(a=1, b=2)
            >>> c1 is c2
            True

        You can pass an additional ``generate_kwargs`` function. This
        function is called only if a new infrastructure is instantiated,
        and should return a dictionary with additional options. These
        additional options are added to ``kwargs`` before instantiating
        a new infrastructure, but are not taken into account to
        determine parameter uniqueness. This is mainly used to setup
        additional resources needed by the infrastructure, like
        temporary directories.

        >>> from unittest.mock import MagicMock
        ...
        >>> class C(Infrastructure):
        ...     def __init__(self, a, b, message):
        ...         self._cluster = MagicMock(name="cluster")
        ...
        ...         print(message)
        ...         self.a = a
        ...         self.b = b
        ...         super().__init__(_ClientClass=MagicMock())
        ...
        ...     @classmethod
        ...     def compute_fingerprint(cls, a, b):
        ...         return hash((cls, a, b))
        ...
        ...     def __hash__(self):
        ...         return self.compute_fingerprint(self.a, self.b)
        ...
        ...     def get_cluster(self):
        ...         return self._cluster
        ...
        >>> from datetime import datetime
        ...
        >>> C.get_unique(
        ...     a=1,
        ...     b=2,
        ...     generate_kwargs=lambda: {
        ...         "message": f"Created at {datetime.now()}."
        ...     },
        ... )
        Created at ...
        <....C object at 0x...>
        >>> C.get_unique(
        ...     a=1,
        ...     b=2,
        ...     generate_kwargs=lambda: {
        ...         "message": f"Created at {datetime.now()}."
        ...     },
        ... )
        <....C object at 0x...>

        If an infrastructure object falls out of scope, it should be
        garbage collected. In this case, getting a new infrastructure
        with the same parameters should instantiate a new object.
        """
        fingerprint: Fingerprint = cls.compute_fingerprint(**kwargs)

        with cls._unique_instances_lock:
            self: _Optional["Infrastructure"] = cls._unique_instances.get(fingerprint)

            if self is None:
                self = cls(**generate_kwargs(), **kwargs)
                cls._unique_instances[fingerprint] = self

            else:
                pass

        return self

    @classmethod
    def _get_cache_length(cls):
        """Return the amount of cached infrastructures.

        Example::

            >>> from tempfile import TemporaryDirectory
            >>> from distributed import LocalCluster
            >>> from gc import collect
            ...
            >>> class C(Infrastructure):
            ...     def __init__(self):
            ...         self._cluster = LocalCluster(
            ...             dashboard_address=None,
            ...             local_directory=TemporaryDirectory().name,
            ...         )
            ...
            ...         super().__init__()
            ...
            ...     @classmethod
            ...     def compute_fingerprint(cls):
            ...         return hash(cls)
            ...
            ...     def __hash__(self):
            ...         return self.compute_fingerprint()
            ...
            ...     def get_cluster(self):
            ...         return self._cluster
            ...
            >>> c = C.get_unique()
            >>> Infrastructure._get_cache_length()
            1
            >>> del c
            >>> _ = collect()
            >>> Infrastructure._get_cache_length()
            0
        """
        return len(cls._unique_instances)

    def submit(
        self,
        *args,
        name="",
        metadata=None,
        status_file: _Optional[PathOrStr] = None,
        tags=None,
        **kwargs,
    ):
        """Submit a task and returns a class:`Task` object.

        A class:`Task` object encapsulates the state of a running task,
        allowing to control the task and to access its metadata.

        Example::

            >>> from tempfile import TemporaryDirectory
            >>> from distributed import LocalCluster
            ...
            >>> class C(Infrastructure):
            ...     def __init__(self):
            ...         self._cluster = LocalCluster(
            ...             dashboard_address=None,
            ...             local_directory=TemporaryDirectory().name,
            ...         )
            ...
            ...         super().__init__()
            ...
            ...     @classmethod
            ...     def compute_fingerprint(cls):
            ...         return hash(cls)
            ...
            ...     def __hash__(self):
            ...         return self.compute_fingerprint()
            ...
            ...     def get_cluster(self):
            ...         return self._cluster
            ...
            >>> c = C.get_unique()
            >>> task = c.submit(lambda: "Success!")
            >>> task.get_result()
            'Success!'

        See :meth:`distributed.Client.submit` for detailed documentation
        about this function's parameters.
        """
        from ..task import Task as _Task  # avoid circular references

        if _is_dask_collection(args[0]):
            with self._client_lock:
                future = self._client.compute(*args, **kwargs)

        else:
            with self._client_lock:
                future = self._client.submit(*args, **kwargs)

        return _Task(
            future,
            infrastructure=self,
            metadata=metadata,
            name=name,
            status_file=status_file,
            submit_time=_DateTime.now(),
            tags=tags,
        )

    def set_metadata(self, keys, value):
        """Set metadata on this infrastructure."""
        with self._client_lock:
            self._client.set_metadata(keys, value)

    def get_metadata(self, keys, default=_no_default):
        """Get metadata on this infrastructure."""
        with self._client_lock:
            result = self._client.get_metadata(keys, default)

        if result is _no_default:
            raise KeyError(f"{keys} was not found in metadata.")

        else:
            return result

    @_safe_future_argument
    def get_result(self, future: Future):
        """Return the results of the execution of a future.

        .. note:: This is a blocking function.
        """
        return future.result()

    @property
    def scheduler_address(self):
        """Return the scheduler's address of this infrastructure.

        This can be used to create more clients connected to this
        infrastructure using the low-level API.
        """
        with self._client_lock:
            return self._client.scheduler.address

    @property
    def dashboard_address(self):
        """Return the dashboard address of this infrastructure."""
        with self._client_lock:
            return self._client.dashboard_link

    def run_on_scheduler(self, callable):
        """Run a function in the scheduler process.

        The signature of ``callable`` should be:
        ``<Signature (dask_scheduler) -> Any>``.
        """
        with self._client_lock:
            return self._client.run_on_scheduler(callable)

    def sync(self, *args, **kwargs):
        """Run a coroutine in the cluster's loop."""
        with self._cluster_lock:
            return self.get_cluster().sync(*args, **kwargs)

    def scale(self, n):
        """Force a cluster to scale up to a fixed number of workers."""
        with self._cluster_lock:
            self.get_cluster().scale(n)

    @_safe_future_argument
    def retry(self, future: Future):
        """Retry a future.

        Use this when a task is lost.
        """
        future.retry()

    def _generic_has_what(
        self,
        method_name: _Literal["has_what", "processing"],  # noqa: F821
        workers: _Iterable[str] | None = None,
    ) -> dict[str, list[str]]:
        with self._client_lock:
            method = getattr(self._client, method_name)
            arguments = () if workers is None else ([*workers],)
            return method(*arguments)

    _has_what = _partialmethod(_generic_has_what, "has_what")
    _processing = _partialmethod(_generic_has_what, "processing")

    def _filter_workers_by_future(
        self, future: Future, workers: _Mapping[str, _Iterable[str]]
    ) -> _Generator[str, None, None]:
        """Filter workers that are processing or have a future's result."""
        for worker, keys in workers.items():
            if future.key in keys:
                yield worker

    def who_has_future(self, future: Future) -> _Generator[str, None, None]:
        """Generate workers that are processing or have a future result."""
        yield from self._filter_workers_by_future(future, self._has_what())
        yield from self._filter_workers_by_future(future, self._processing())

    def run_on_workers(self, worker_addresses, function, *args, **kwargs):
        """Run ``function`` on workers in ``worker_addresses``."""
        with self._client_lock:
            return self._client.run(
                function, *args, workers=[*worker_addresses], **kwargs
            )

    def get_job_ids(self, future: Future) -> _Tuple:
        """Return tuple of job ids involved in processing ``future``."""
        raise NotImplementedError

    def get_host_names(
        self, future: Future, _host_name_function=_get_host_name
    ) -> _Tuple[str, ...]:
        """Return a tuple of hosts involved in processing ``future``."""
        workers = self.who_has_future(future)
        host_names = self.run_on_workers(workers, _host_name_function).values()
        return tuple(host_names)

    @_safe_future_argument
    def cancel(self, future: Future):
        """Cancel a future in a thread-safe way.

        Example::

            >>> from rayuela import submit_command
            >>>
            >>> task = submit_command('sleep 60')
            >>> task
            <Task: name='', key='...', status=<Status...: '...'>>
            >>> task.cancel()
            >>> task
            <Task: name='', key='...', status=<Status.CANCELLED: 'cancelled'>>
        """
        future.cancel()

    def _get_job_script(self, cluster):
        with self._cluster_lock:
            try:
                return cluster.job_script()

            except AttributeError as ae:
                if not hasattr(cluster, "job_script"):
                    raise RuntimeError(
                        "This infrastructure does not use job scripts to launch jobs."
                    ) from ae

                else:
                    raise

    @property
    def job_script(self):
        """Return the job script used to launch jobs on this infrastructure.

        :raises RuntimeError:
            if the infrastructure does not launch jobs using job scripts.
        """
        with self._cluster_lock:
            cluster = self.get_cluster()
            return self._get_job_script(cluster)


# ****************************************
# AppDigest signature for component RAYUELA. Do not delete!
# ****************************************
# @(&)Synopsys_Unique_Signature Libname="infrastructure.py" Name="RAYUELA" Version="4.0.0" Type="REUSE" Injector="V-2023.09-DEV (8729328)"(&)@
