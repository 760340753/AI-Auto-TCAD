"""CDPL infrastructure."""


from __future__ import annotations


from logging import getLogger as _get_logger  # noqa: N813


try:
    from guanaco import (  # type: ignore
        CDPLInfraSpec as _CDPLInfraSpec,
        CDPLMaster as _CDPLMaster,
        CDPLWorker as _CDPLWorker,
        CDPLWorkerSpec as _CDPLWorkerSpec,
    )

except Exception:
    _get_logger(__name__).warning(
        "Could not import Guanaco. CDPL infrastructure will not be available."
    )

else:
    from asyncio import get_event_loop as _get_event_loop
    from os import PathLike as _PathLike
    from sys import executable as _executable

    from typing import (
        Any as _Any,
        Mapping as _Mapping,
    )

    from warnings import (
        catch_warnings as _catch_warnings,
        simplefilter as _simplefilter,
    )

    from weakref import finalize as _finalize

    from distributed import (  # type: ignore
        Nanny as _Nanny,
        rpc as _rpc,
        Scheduler as _Scheduler,
    )

    from distributed.deploy.spec import (  # type: ignore
        ProcessInterface as _ProcessInterface,
        SpecCluster as _SpecCluster,
    )

    from ..infrastructure import Infrastructure as _Infrastructure
    from ..mixins import AdaptOnFirstSubmit as _AdaptOnFirstSubmit
    from ..utils import recursive_to_hashable as _recursive_to_hashable

    _logger = _get_logger(__name__)

    class _RemoteDaskCDPLWorker(_CDPLWorker):
        """Remote Guanaco worker."""

        async def run_dask_worker(self, *args, **kwargs):
            """Run a Dask worker.

            :param args: Positional arguments to pass to the worker.
            :param kwargs: Keyword arguments to pass to the worker.
            """
            _logger.debug("Starting Dask worker.")

            async with _Nanny(*args, **kwargs) as nanny:
                _logger.debug("Dask worker started.")
                await nanny.finished()

            _logger.debug("Dask worker finished.")

        def start_dask_worker(self, *args, **kwargs):
            """Start a Dask worker.

            :param args: Positional arguments to pass to the worker.
            :param kwargs: Keyword arguments to pass to the worker.
            """
            _get_event_loop().run_until_complete(self.run_dask_worker(*args, **kwargs))

    class _LocalWorkerInterface(_ProcessInterface):
        """Interface to a Dask worker running on a CDPL worker."""

        def __init__(
            self,
            scheduler: _Scheduler | None = None,
            name=None,
            cdpl_master: _CDPLMaster | None = None,
            dask_worker_args=None,
        ):
            """Initialize the worker.

            :param scheduler: The scheduler address.
            :param name: The worker name.
            :param cdpl_master: The CDPL master.

            :param dask_worker_args:
                Arguments to pass to the Dask worker.

            :raises ValueError: If the CDPL master is not specified.
            """
            if cdpl_master is None:
                raise ValueError("The CDPL master must be specified.")

            self._cdpl_master = cdpl_master

            self._dask_worker_args = {
                "scheduler_ip": scheduler,
                "name": name,
                **(dask_worker_args or {}),
            }

            self._scheduler_comm = _rpc(scheduler)
            self.name = name
            super().__init__(scheduler, name)

        async def start(self):
            """Submit the process to the resource manager.

            For workers this doesn't have to wait until the process
            actually starts, but can return once the resource manager
            has the request, and will work to make the job exist in the
            future.
            """
            self._cdpl_master.max_workers += 1

            self._cdpl_master.create_task(
                "start_dask_worker", (), self._dask_worker_args
            )
            """.. note:: This is a workaround for a bug in Guanaco that
            is currently being processed in the Guanaco pipeline. In the
            future this should be replaced by the following code:

            .. code-block:: python

                self._cdpl_master.send_task("start_dask_worker", **self._dask_worker_args)
            """  # noqa: E501

            await super().start()

        async def close(self):
            """Close the process.

            This will be called by the Cluster object when we scale down
            a node, but only after we ask the Scheduler to close the
            worker gracefully. This method should kill the process a bit
            more forcefully and does not need to worry about shutting
            down gracefully.
            """
            self._cdpl_master.max_workers -= 1

            await self._scheduler_comm.retire_workers(
                workers=[self.name], close_workers=True
            )

            await super().close()

    class CDPLInfrastructure(_AdaptOnFirstSubmit, _Infrastructure):
        """CDPL infrastructure."""

        def __init__(
            self,
            cdpl_infra_spec: _CDPLInfraSpec | None = None,
            dask_worker_args: _Mapping[str, _Any] | None = None,
            remote_python_interpreter: _PathLike[str] | str | None = None,
            **kwargs,
        ):
            """Set up client and cluster.

            :param cdpl_infra_spec:
                The specification of the CDPL infrastructure to use. If
                not specified, a shell on the local machine is used.

            :param dask_worker_args:
                Keyword arguments to pass to the Dask worker. These
                arguments are passed to the initializer of the
                :class:`~distributed.Nanny` class.

            :param remote_python_interpreter:
                The path to the Python interpreter to use on the remote
                machine. If not specified, the Python interpreter used
                to start the current process is used.

            :param kwargs:
                Keyword arguments used to support collaborative multiple
                inheritance. These arguments are not used by this class,
                but they are passed to the initializer of the parent
                class.

            :class:`Infrastructure` is an abstraction over a
            :class:`distributed.Client` and a
            :class:`distributed.deploy.Cluster`. This initializer sets
            up these resources and makes sure they are cleaned up
            appropriately when instances of this class are no longer
            needed.

            Since the classes inheriting from :class:`Infrastructure`
            should provide a :class:`~distributed.deploy.Cluster`
            instance via :meth:`get_cluster`, this method should be
            called (``super().__init__()``) after initializing the
            :class:`~distributed.deploy.Cluster` instance.
            """
            self._cdpl_infra_spec = cdpl_infra_spec or _CDPLInfraSpec()
            self._dask_worker_args = dask_worker_args or {}
            self._remote_python_interpreter = remote_python_interpreter or _executable

            worker_spec = _CDPLWorkerSpec(
                f"{self._remote_python_interpreter} -m rayuela.infrastructures.cdpl",
                min_=0,
                max_=1,
            )

            self._master = _CDPLMaster(worker_spec, self._cdpl_infra_spec)
            _finalize(self, self._master.stop)

            with _catch_warnings():
                _simplefilter("ignore")

                self._cluster = _SpecCluster(
                    silence_logs="ERROR",
                    worker={
                        "cls": _LocalWorkerInterface,
                        "options": {
                            "cdpl_master": self._master,
                            "dask_worker_args": self._dask_worker_args,
                        },
                    },
                )

            super().__init__(**kwargs)

        def get_cluster(self):
            """Return the cluster.

            :return: The cluster.
            """
            return self._cluster

        @classmethod
        def compute_fingerprint(  # type: ignore
            cls,
            cdpl_infra_spec: _CDPLInfraSpec | None = None,
            dask_worker_args: _Mapping[str, _Any] | None = None,
            remote_python_interpreter: _PathLike[str] | str | None = None,
            max_cdpl_workers: int = 1000,
            **kwargs,
        ):
            """Compute the fingerprint of the infrastructure.

            :param cdpl_infra_spec:
                The specification of the CDPL infrastructure to use. If
                not specified, a shell on the local machine is used.

            :param dask_worker_args:
                Keyword arguments to pass to the Dask worker. These
                arguments are passed to the initializer of the
                :class:`~distributed.Nanny` class.

            :param remote_python_interpreter:
                The path to the Python interpreter to use on the remote
                machine. If not specified, the Python interpreter used
                to start the current process is used.

            :param max_cdpl_workers:
                The maximum number of CDPL workers to start. This is a
                required argument for CDPL. The default value is 1000.

            :param kwargs:
                Keyword arguments used to support collaborative multiple
                inheritance. These arguments are not used by this class,
                but they are passed to the initializer of the parent
                class.

            :return: The fingerprint.
            """
            return hash(
                (
                    super().compute_fingerprint(**kwargs),
                    _recursive_to_hashable(cdpl_infra_spec),
                    _recursive_to_hashable(dask_worker_args),
                    remote_python_interpreter,
                    max_cdpl_workers,
                )
            )

        def __hash__(self):
            """Compute the hash of the infrastructure.

            :return: The hash.
            """
            return self.compute_fingerprint(
                self._cdpl_infra_spec,
                self._dask_worker_args,
                self._remote_python_interpreter,
                self._max_cdpl_workers,
            )


# ****************************************
# AppDigest signature for component RAYUELA. Do not delete!
# ****************************************
# @(&)Synopsys_Unique_Signature Libname="__init__.py" Name="RAYUELA" Version="4.0.0" Type="REUSE" Injector="V-2023.09-DEV (8729328)"(&)@
